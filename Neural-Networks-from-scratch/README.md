## Implemented Neural Network for classification with mini batch gradient descent using numpy<br>
Dataset: MNIST Handwritten digit images (0-9) <br>
Training images: 50,000 <br>
Testing images: 10,000 <br>
Image size: 28x28 <br>

## Architecture
### 1 hidden layer
### Loss function: Cross-entropy loss
### Activation function:
- Sigmoid (Hidden layer)
- Softmax (Output layer)
## Hyperparameters
### Learning Rate: 5
### Number of hidden units: 300
### Batch size: 1000
### Epochs: 30
